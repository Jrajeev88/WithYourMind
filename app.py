import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig
from deep_translator import GoogleTranslator
import torch
import os

# Create offload directory if it doesn't exist, needed by device_map="auto"
os.makedirs("offload", exist_ok=True)

# --- Model Loading ---
# Path to your fine-tuned model. Make sure this directory contains
# your saved model (e.g., adapter_model.bin) and tokenizer files.
# If deploying to Hugging Face Spaces, you would typically upload this directory
# as part of your repository, or specify a Hugging Face model ID.
FINE_TUNED_MODEL_PATH = "./fine_tuned_model"

# Configure BitsAndBytes for 4-bit quantization if your saved model is quantized.
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False,
    llm_int8_enable_fp32_cpu_offload=True # Needed if parts are offloaded to CPU
)

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    FINE_TUNED_MODEL_PATH,
    quantization_config=bnb_config,
    device_map="auto", # Automatically distributes model layers across available devices
    offload_folder="offload", # Directory for CPU offloading
    low_cpu_mem_usage=True # Reduces CPU RAM usage during loading
)
model.eval() # Set model to evaluation mode
if torch.cuda.is_available():
    model.to("cuda") # Ensure model is on GPU if available

# Set generation configuration for consistent outputs
model.generation_config = GenerationConfig(
    max_new_tokens=180,
    temperature=0.7,
    top_p=0.85,
    do_sample=True,
    repetition_penalty=1.25,
    pad_token_id=tokenizer.eos_token_id,
    eos_token_id=tokenizer.eos_token_id
)

# --- Translation and Crisis Functions ---
translator = GoogleTranslator(source='auto', target='en')

malayalam_chars = set("‡¥Ö‡¥Ü‡¥á‡¥à‡¥â‡¥ä‡¥ã‡¥é‡¥è‡¥ê‡¥í‡¥ì‡¥î‡¥ï‡¥ñ‡¥ó‡¥ò‡¥ô‡¥ö‡¥õ‡¥ú‡¥û‡¥ü‡¥†‡¥°‡¥¢‡¥£‡¥§‡¥•‡¥¶‡¥ß‡¥®‡¥™‡¥´‡¥¨‡¥≠‡¥Æ‡¥Ø‡¥∞‡¥≤‡¥µ‡¥∂‡¥∑‡¥∏‡¥π‡¥≥‡¥¥‡¥±")
def is_malayalam(text):
    return any(ch in malayalam_chars for ch in text)

def emergency_response_ml():
    return (
        "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥™‡¥ô‡µç‡¥ï‡µÅ‡¥µ‡µÜ‡¥ö‡µç‡¥ö‡¥§‡µç ‡¥µ‡¥≥‡¥∞‡µÜ ‡¥™‡µç‡¥∞‡¥ß‡¥æ‡¥®‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü‡¥§‡¥æ‡¥£‡µç. ‡¥û‡¥æ‡µª ‡¥á‡¥µ‡¥ø‡¥ü‡µÜ ‡¥ï‡µá‡µæ‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥Ü‡¥£‡µç, "
        "‡¥é‡¥®‡µç‡¥®‡¥æ‡µΩ ‡¥Ö‡¥ü‡¥ø‡¥Ø‡¥®‡µç‡¥§‡¥ø‡¥∞ ‡¥∏‡¥æ‡¥π‡¥ö‡¥∞‡µç‡¥Ø‡¥ô‡µç‡¥ô‡¥≥‡¥ø‡µΩ ‡¥®‡µá‡¥∞‡¥ø‡¥ü‡µç‡¥ü‡µç ‡¥∏‡¥π‡¥æ‡¥Ø‡¥Ç ‡¥®‡µΩ‡¥ï‡¥æ‡µª ‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥ï‡¥¥‡¥ø‡¥Ø‡¥ø‡¥≤‡µç‡¥≤.\n\n"
        "üìç **‡¥á‡¥®‡µç‡¥§‡µç‡¥Ø‡¥Ø‡¥ø‡¥≤‡µÜ ‡¥Ö‡¥ü‡¥ø‡¥Ø‡¥®‡µç‡¥§‡¥ø‡¥∞ ‡¥∏‡¥π‡¥æ‡¥Ø ‡¥®‡¥Æ‡µç‡¥™‡¥±‡µÅ‡¥ï‡µæ:**\n"
        "‚Ä¢ AASRA (24/7): 022 2754 6669\n"
        "‚Ä¢ ‡¥∏‡µº‡¥ï‡µç‡¥ï‡¥æ‡µº KIRAN ‡¥π‡µÜ‡µΩ‡¥™‡µç‡¥≤‡µà‡µª: 1800-599-0019\n"
        "‚Ä¢ iCALL (WhatsApp/‡¥ö‡¥æ‡¥±‡µç‡¥±‡µç): +91 9152987821\n\n"
        ". Tele Manas: 14416\n"
        "üìå ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥í‡¥∞‡µÅ ‡¥Æ‡¥®‡µÅ‡¥∑‡µç‡¥Ø‡¥®‡µÅ‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥á‡¥™‡µç‡¥™‡µã‡µæ ‡¥§‡¥®‡µç‡¥®‡µÜ ‡¥¨‡¥®‡µç‡¥ß‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥ï.\n"
        "üìå ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥í‡¥±‡µç‡¥±‡¥ï‡µç‡¥ï‡¥≤‡µç‡¥≤.\n"
    )

def emergency_response_en():
    return (
        "Thank you for saying that. I‚Äôm here to listen, but I can‚Äôt provide emergency help.\n\n"
        "üìç **India-specific crisis support:**\n"
        "‚Ä¢ AASRA (24/7 Suicide Prevention): 022 2754 6669\n"
        "‚Ä¢ Govt. KIRAN Mental Health Helpline: 1800-599-0019\n"
        "‚Ä¢ iCALL WhatsApp Support: +91 9152987821\n\n"
        ". Tele Manas: 14416\n"
        "If you're in immediate danger, please contact a human right now.\n"
        "You are not alone.\n"
    )

def check_crisis_status(user_input):
    crisis_terms = [
        "suicide", "kill myself", "end my life", "self harm",
        "can't go on", "want to die", "hopeless", "worthless",
        "no reason to live", "take my life", "harm myself", "give up",
        "feel like dying", "don't want to live", "better off dead",
        "suicidal thoughts", "ending it all"
    ]

    if any(w in user_input.lower() for w in crisis_terms):
        return emergency_response_en()
    return None

def naturalize_malayalam(text):
    replacements = {
        "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥í‡¥∞‡¥æ‡¥≥‡¥≤‡µç‡¥≤": "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥í‡¥±‡µç‡¥±‡¥ï‡µç‡¥ï‡¥≤‡µç‡¥≤",
        "‡¥í‡¥∞‡µÅ ‡¥§‡µÄ‡¥∞‡µÅ‡¥Æ‡¥æ‡¥®‡¥Ç ‡¥é‡¥ü‡µÅ‡¥ï‡µç‡¥ï‡¥£‡¥Ç": "‡¥í‡¥∞‡µÅ ‡¥§‡µÄ‡¥∞‡µÅ‡¥Æ‡¥æ‡¥®‡¥§‡µç‡¥§‡¥ø‡¥≤‡µá‡¥ï‡µç‡¥ï‡µç ‡¥é‡¥§‡µç‡¥§‡¥£‡¥Ç",
        "‡¥û‡¥æ‡µª ‡¥á‡¥µ‡¥ø‡¥ü‡µÜ ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µã‡¥ü‡µä‡¥™‡µç‡¥™‡¥Ç": "‡¥û‡¥æ‡µª ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µã‡¥ü‡µä‡¥™‡µç‡¥™‡¥Ç ‡¥â‡¥£‡µç‡¥ü‡µÜ‡¥®‡µç‡¥®‡µç ‡¥ï‡¥∞‡µÅ‡¥§‡µÅ‡¥ï",
        "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥Ö‡¥§‡µç ‡¥¨‡µÅ‡¥¶‡µç‡¥ß‡¥ø‡¥Æ‡µÅ‡¥ü‡µç‡¥ü‡¥æ‡¥ï‡¥æ‡¥Ç": "‡¥Ö‡¥§‡µç ‡¥µ‡µÜ‡¥±‡µÅ‡¥Ç ‡¥é‡¥≥‡µÅ‡¥™‡µç‡¥™‡¥Æ‡¥≤‡µç‡¥≤‡µÜ‡¥®‡µç‡¥®‡µç ‡¥§‡µã‡¥®‡µç‡¥®‡µÅ‡¥®‡µç‡¥®‡µÅ",
        "‡¥ï‡¥†‡¥ø‡¥®‡¥Æ‡¥æ‡¥£‡µç": "‡¥¨‡µÅ‡¥¶‡µç‡¥ß‡¥ø‡¥Æ‡µÅ‡¥ü‡µç‡¥ü‡¥æ‡¥Ø‡¥ø ‡¥§‡µã‡¥®‡µç‡¥®‡µÅ‡¥®‡µç‡¥®‡µÅ",
        "‡¥Ü‡¥∂‡¥Ø‡¥ï‡µç‡¥ï‡µÅ‡¥¥‡¥™‡µç‡¥™‡¥Ç": "‡¥ï‡µÅ‡¥¥‡¥™‡µç‡¥™‡¥Ç",
        "‡¥∏‡¥π‡¥æ‡¥Ø‡¥Ç ‡¥§‡µá‡¥ü‡µÅ‡¥ï": "‡¥Ü‡¥≥‡µÅ‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥∏‡¥Ç‡¥∏‡¥æ‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï"
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    return text

def withyourmind_chat(user_input, history=""):
    prompt = f"""
You are WithYourMind, a non-clinical emotional support companion.\nRespond with empathy, reflection, and grounding. Do not diagnose or claim professional authority.\n\nConversation so far:\n{history}\n\nUser: {user_input}\nWithYourMind:\n""".strip()

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    output = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=True,
        temperature=model.generation_config.temperature,
        top_p=model.generation_config.top_p,
        repetition_penalty=model.generation_config.repetition_penalty
    )

    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response.split("WithYourMind:")[-1].strip()

def withyourmind_ml(user_input, history=""):
    crisis_terms_ml = ["‡¥Ü‡¥§‡µç‡¥Æ‡¥π‡¥§‡µç‡¥Ø","‡¥Ü‡¥§‡µç‡¥Æ‡¥π‡¥§‡µç‡¥Ø‡¥æ‡¥™‡¥∞‡¥Æ‡¥æ‡¥Ø","‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥Ç ‡¥Ö‡¥µ‡¥∏‡¥æ‡¥®‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï","‡¥™‡µã‡¥ï‡¥£‡¥Ç","‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µÅ ‡¥ú‡µÄ‡¥µ‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥®‡¥ø‡¥≤‡µç‡¥≤"]
    if any(term in user_input for term in crisis_terms_ml):
        return emergency_response_ml()

    if is_malayalam(user_input):
        en = translator.translate(user_input)
        reply_en = withyourmind_chat(en, history)
        reply_ml = GoogleTranslator(source='en', target='ml').translate(reply_en)
        return naturalize_malayalam(reply_ml)
    return withyourmind_chat(user_input, history)

# --- Gradio Interface Function ---
def gradio_chat(user_input, history):
    if history is None:
        history = []

    english_crisis_response = check_crisis_status(user_input)
    if english_crisis_response:
        history.append((user_input, english_crisis_response))
        return history

    if is_malayalam(user_input):
        reply = withyourmind_ml(user_input, history)
        history.append((user_input, reply))
        return history

    reply = withyourmind_chat(user_input, history)
    history.append((user_input, reply))
    return history

# --- Gradio Blocks Setup ---
if __name__ == "__main__":
    with gr.Blocks(theme="soft") as iface:
        gr.Markdown("# ü§ù WithYourMind ‚Äî Emotional Support Chatbot")
        gr.Markdown("Non-clinical emotional support ‚Ä¢ Empathetic responses ‚Ä¢ Crisis-safe boundaries")

        chatbot = gr.Chatbot(height=450)
        msg = gr.Textbox(label="How are you feeling today?", placeholder="Type here...")

        msg.submit(gradio_chat, [msg, chatbot], chatbot)
        msg.submit(lambda x: "", msg)

    iface.launch(share=True)
